# -*- coding: utf-8 -*-
"""bank_churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yc0wTo4ge7w_arlA26fJMwHbcuw7defv

**Customer Churn prediction:-**

 Customer Churn prediction means knowing which customers are likely to leave or unsubscribe from your service. For many companies, this is an important prediction. This is because acquiring new customers often costs more than retaining existing ones. Once youâ€™ve identified customers at risk of churn, you need to know exactly what marketing efforts you should make with each customer to maximize their likelihood of staying.




Customers have different behaviors and preferences, and reasons for cancelling their subscriptions. Therefore, it is important to actively communicate with each of them to keep them on your customer list. You need to know which marketing activities are most effective for individual customers and when they are most effective.
"""

# importing the require libarary
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

#importing the drive from the google
from google.colab import drive
drive.mount('/content/drive')

#load the data sheet from the drive
churn= pd.read_csv("/content/drive/MyDrive/ZummitAfrica(AI ML Learning Path)/1_project customer churn prediction /Churn_Modelling.csv")

# Display information about the dataset, including data types and missing values
churn.info()

# Display information about the dataset, including data types and missing values
churn.info()

# Display summary statistics of the dataset
churn.describe()

"""#Auto EDA process"""

'''!pip install matplotlib
!pip install pandas-profiling
!pip install --upgrade pandas
!pip install --upgrade numpy
!pip install --upgrade matplotlib
!pip install --upgrade seaborn
!pip install --upgrade scikit-learn
!pip install sweetviz'''

"""Restart the kernal once again"""

import matplotlib.pyplot as plt
import sweetviz as sv

# Analyze the dataset
report = sv.analyze(churn)

# Display the report
report.show_html("churn_eda_report.html")

"""#EDA  (Exploratory Data Analysis)"""

# Get the dimensions of the dataset (rows, columns)
churn.shape

# Get the total number of elements in the dataset
churn.size

# Get the names of the columns in the dataset
churn.columns

# Count missing values in each column
churn.isnull().sum()

# Identify and display duplicate rows based on the 'customer_id' column
duplicates = churn[churn.duplicated(subset=['CustomerId'], keep=False)]
print(duplicates)

"""# Data Visualization"""

# Set the figure size for the following visualization
plt.figure(figsize=(15, 5))

# Create a count plot to visualize the distribution of the 'exited' variable in the original dataset
sns.countplot(data=churn, x='Exited')

# Display the count of each class in the 'exited' variable
churn['Exited'].value_counts().to_frame()

# Class Imbalance Resampling
# Select the majority and minority classes
churn_majority = churn[churn['Exited'] == 0]
churn_minority = churn[churn['Exited'] == 1]

from sklearn.utils import resample

# Downsample the majority class to match the size of the minority class
churn_majority_downsample = resample(churn_majority, n_samples=2037, replace=False, random_state=42)

# Combine the resampled majority class with the minority class
churn_df = pd.concat([churn_majority_downsample, churn_minority])

# Set the figure size for the following visualization
plt.figure(figsize=(15, 5))

# Create a count plot to visualize the distribution of the 'exited' variable in the resampled dataset
sns.countplot(data=churn_df, x='Exited')

# Display the column names in the 'churn_df' DataFrame
churn_df.columns

# Remove specific columns from the 'churn_df' DataFrame
# These columns include 'rownumber,' 'customerid,' 'surname,' 'geography,' and 'gender.'
churn_df.drop(['RowNumber', 'CustomerId', 'Surname', 'Geography','Gender',], axis=1, inplace=True)

# Compute the correlation matrix for the remaining columns in the 'churn_df' DataFrame
churn_df.corr()

# Create a heatmap to visualize the correlation between different features
# The 'annot=True' parameter adds values to the heatmap
plt.figure(figsize=(15, 5))
sns.heatmap(churn_df.corr(), annot=True)

# Calculate the correlation of each feature with the 'exited' variable and store it in 'df_corr_exit'
df_corr_exit = churn_df.corr()['Exited'].to_frame()

# Create a bar plot to visualize the correlation of each feature with the 'exited' variable
plt.figure(figsize=(15, 5))
sns.barplot(data=df_corr_exit, x=df_corr_exit.index, y='Exited')

# Separate the feature columns (independent variables) into 'x'
x = churn_df.drop(['Exited'], axis=1)
# Separate the target variable ('exited') into 'y'
y = churn_df['Exited']

"""#Spliting the Data Set"""

# Import the necessary function from scikit-learn
from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
# x: Features (independent variables)
# y: Target variable (dependent variable)
# test_size: The proportion of the data to include in the test split (in this case, 30% for testing)
# random_state: A random seed for reproducibility
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Print the dimensions of the resulting datasets
x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""#Modeling and Evaluation"""

# Import the logistic regression model from scikit-learn
from sklearn.linear_model import LogisticRegression

# Create a logistic regression model with a specified maximum number of iterations
lr = LogisticRegression(max_iter=500)

# Train the logistic regression model on the training data
lr.fit(x_train, y_train)

# Calculate the accuracy score on the training set
train_accuracy = lr.score(x_train, y_train)
print("Training Accuracy:", train_accuracy)

# Predict outcomes on the test set
y_pred = lr.predict(x_test)

# Import necessary functions for performance evaluation
from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, f1_score, ConfusionMatrixDisplay

# Calculate the precision score on the test set
test_precision = precision_score(y_test, y_pred)
print("Test Precision Score:", test_precision)

# Calculate the recall score on the test set
test_recall = recall_score(y_test, y_pred)
print("Test Recall Score:", test_recall)

# Calculate the accuracy score on the test set
test_accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy Score:", test_accuracy)

# Calculate the F1 score on the test set
test_f1 = f1_score(y_test, y_pred)
print("Test F1 Score:", test_f1)

# Create a ConfusionMatrixDisplay object for visualization
cmd = ConfusionMatrixDisplay(
    confusion_matrix=confusion_matrix(y_test, y_pred, labels=lr.classes_),
    display_labels=lr.classes_
)
# Plot the confusion matrix
cmd.plot()

"""#k-Nearest Neighbors (KNN)"""

# Import and create a KNN classifier with k=3
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)

# Train the KNN model on the training data
knn.fit(x_train, y_train)

# Calculate the accuracy score on the training set for KNN
knn_train_accuracy = knn.score(x_train, y_train)
print("KNN Training Accuracy:", knn_train_accuracy)

# Calculate the accuracy score on the test set for KNN
knn_test_accuracy = knn.score(x_test, y_test)
print("KNN Test Accuracy:", knn_test_accuracy)

"""#Support Vector Classifier (SVC):"""

# Import and create an SVC classifier
from sklearn.svm import SVC
svc = SVC()

# Train the SVC model on the training data
svc.fit(x_train, y_train)

# Calculate the accuracy score on the training set for SVC
svc_train_accuracy = svc.score(x_train, y_train)
print("SVC Training Accuracy:", svc_train_accuracy)

# Calculate the accuracy score on the test set for SVC
svc_test_accuracy = svc.score(x_test, y_test)
print("SVC Test Accuracy:", svc_test_accuracy)

import pickle

# Save the trained model (e.g., 'knn') to a pickle file
with open('trained_model.pkl', 'wb') as file:
    pickle.dump(knn, file)

from google.colab import files

# Download the pickle file
files.download('trained_model.pkl')



!pip install streamlit

import streamlit as st
import pickle  # To load the trained model
import numpy as np

# Load the trained model
with open('trained_model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

# Define the Streamlit app
st.title("Machine Learning Model Deployment")

# Create input elements (e.g., text input, sliders)
feature1 = st.slider("Feature 1", 0.0, 10.0)
feature2 = st.slider("Feature 2", 0.0, 10.0)

# Create a button to make predictions
if st.button("Predict"):
    # Make predictions using the loaded model
    input_data = np.array([feature1, feature2]).reshape(1, -1)
    prediction = model.predict(input_data)
    st.write(f"Prediction: {prediction[0]}")